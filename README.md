# Ollama Local API

Esta API permite o gerenciamento e interaÃ§Ã£o com modelos de linguagem como LLaMA, Qwen e outros modelos hospedados localmente via Ollama. O projeto foi desenvolvido para facilitar a execuÃ§Ã£o e gerenciamento de modelos em ambientes locais, oferecendo funcionalidades como criaÃ§Ã£o, exclusÃ£o, listagem e interaÃ§Ã£o via chat.

ğŸ’¡ BenefÃ­cio Principal: Este projeto Ã© ideal para desenvolvedores que nÃ£o possuem mÃ¡quinas potentes para rodar modelos de linguagem localmente. Utilizando os recursos do Google Colab, Ã© possÃ­vel carregar e interagir com modelos pesados de forma gratuita e sem a necessidade de hardware dedicado.

## ğŸ–¼ï¸ Arquitetura
![Arquitetura do Projeto](./assets/architecture.png)

## ğŸ“‹ Funcionalidades

* **VerificaÃ§Ã£o de SaÃºde** (`/healthcheck`): Verifica se o servidor estÃ¡ ativo.
* **Gerenciamento de Modelos** (`/models`): CriaÃ§Ã£o, listagem e exclusÃ£o de modelos.
* **InteraÃ§Ã£o via Chat** (`/chat`): Envio de mensagens para os modelos.
* **DocumentaÃ§Ã£o OpenAPI** (`/docs`): DocumentaÃ§Ã£o completa da API com suporte ao Swagger.

## ğŸš€ Tecnologias Utilizadas

* **Python 3.12**
* **Flask** - Framework web
* **Flask-Smorest** - DocumentaÃ§Ã£o OpenAPI
* **Marshmallow** - ValidaÃ§Ã£o de dados
* **Ollama SDK** - InteraÃ§Ã£o com modelos de linguagem
* **ThreadPoolExecutor** - Gerenciamento de downloads assÃ­ncronos

## ğŸ“ Estrutura do Projeto

```
.
â”œâ”€â”€ app.py                # ConfiguraÃ§Ã£o principal do Flask
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ config.py        # ConfiguraÃ§Ãµes do servidor
â”‚   â””â”€â”€ logger.py        # ConfiguraÃ§Ã£o do logger
â”œâ”€â”€ models/
â”‚   â””â”€â”€ model_service.py # LÃ³gica dos modelos
â”œâ”€â”€ routes/
â”‚   â”œâ”€â”€ chat.py          # Endpoints para chat
â”‚   â”œâ”€â”€ healthcheck.py   # Endpoint para verificaÃ§Ã£o de saÃºde
â”‚   â””â”€â”€ model.py         # Endpoints para gerenciamento de modelos
â””â”€â”€ schemas/
    â””â”€â”€ schemas.py       # ValidaÃ§Ã£o de dados (Marshmallow)
```

## ğŸ“¦ InstalaÃ§Ã£o

1. Clone o repositÃ³rio:

```bash
git clone https://github.com/seu-usuario/ollama-local-api.git
cd ollama-local-api
```

2. Crie e ative um ambiente virtual:

```bash
python -m venv .venv
source .venv/bin/activate  # No Windows: .venv\Scripts\activate
```

3. Instale as dependÃªncias:

```bash
pip install -r requirements.txt
```

4. Defina as variÃ¡veis de ambiente:

```bash
export OLLAMA_HOST=http://localhost:11400
export MAX_WORKERS=4
```

5. Inicie o servidor:

```bash
python app.py
```

## ğŸ“– DocumentaÃ§Ã£o

Acesse a documentaÃ§Ã£o interativa (Swagger) em:

* **Swagger UI**: [http://localhost:5000/docs/swagger](http://localhost:5000/docs/swagger)

## ğŸ”§ Testes

Para testar a API, vocÃª pode usar o comando **curl** ou qualquer cliente REST, como **Postman** ou **Insomnia**. Aqui estÃ£o alguns exemplos:

### Healthcheck

```bash
curl -X GET http://localhost:5000/healthcheck
```

### CriaÃ§Ã£o de Modelo

```bash
curl -X POST http://localhost:5000/models -H "Content-Type: application/json" -d '{"model": "llama3.1:8b"}'
```

### Listagem de Modelos

```bash
curl -X GET http://localhost:5000/models
```

### RemoÃ§Ã£o de Modelo

```bash
curl -X DELETE http://localhost:5000/models/llama3.1:8b
```

### Chat com o Modelo

```bash
curl -X POST http://localhost:5000/chat -H "Content-Type: application/json" -d '{"model": "llama3.1:8b", "messages": [{"role": "user", "content": "Qual Ã© a capital do Brasil?"}]}'
```

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ licenciado sob a licenÃ§a MIT. Consulte o arquivo [LICENSE](LICENSE) para mais detalhes.

